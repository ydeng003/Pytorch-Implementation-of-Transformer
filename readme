This is a PyTorch implementations of Transformer. 

**************files in the folder***************

train.en and train.de are downloaded from https://nlp.stanford.edu/projects/nmt/
train.en contains the original sentences, train.de contains the target sentences.
vocab.50K.en and vocab.50K.de are vocabulary dictionary for English and German respectively.

orig_eval20.csv, orig_train20.csv, orig_val20.csv, trg_eval20.csv, trg_train20.csv, trg_val20.csv are datasets used for the best model, Model 5.2, indicated in the report.


dataPreproocess.py is used to preprocess the data. orig_eval20.csv, orig_train20.csv, orig_val20.csv, trg_eval20.csv, trg_train20.csv, trg_val20.csv are generated by dataPreproocess.py.

en.json and de.json are dictionary files generated by dataPreproocess.py.

sublayer.py, encoder.py. decoder.py, transformer.py construct the Transformer architecture.

optimScheduler.py is used to provide the optimizer. optimScheduler.py will be called by train_gup_multibatch.py while training to update the weight.

train_gup_multibatch.py is used to train the model. This is the code for Model 5 indicated in the report. Because Model 5 gives us the best BLEU. 1 class and 5 functions are defined in this file. 
Class TransformerDataset is used to prepare dataset to a format that Pytorch can load. Function dataloaders is used to prepare DataLoader which is necessary for Pytorch to load data.
Function train_epoch is used to train an epoch. While training we will record loss, accuracy, time spent for each batch and each epoch, and save it to a file.
Function eval_epoch is used to evaluate a model after each epoch is trained. While eval_epoch is called, we will record loss, accuracy, time spent for each batch and each epoch, and save it to a file. 
Function train will train the model. The number of epochs to be trained is decided by the size of the training dataset, the number of total training steps and the batch size. After training an epoch, we will save this model in case the training process is stopped by some unpredictable reason. We will also save the best model with the least loss for the validation dataset. 
Function main will call function train. After training it will generate two plots which are loss.png and accuracy.png showing the loss and accuracy for both training dataset and validation dataset.

test_gpu.py is used to test the best model saved by the training process. BLEU will be calculated at the end of testing process and printed out on the output file test20_output.txt.

train.sh is used to train the model. 

test.sh is used to test the model.


**************run the model***************

1 Put all the files in the same folder

2 To preprocess the data, run the dataPreprocess.py with any python environment. It will take about 5 hours to finish processing the whole dataset.
To run the code for training and testing only, a sample preprocessed dataset of Model 5.2 are provided in the submission folder as indicated in the previous section. Step 2 can be skipped.

3 To train the model, login to turing.hpc.odu.edu. Copy the whole folder to turing. Run train.sh

4 After training is finished, run test.sh to test the model. BLEU will be printed at the end of the output file test20_output.txt. The targets will be saved to test_tgts.pt, prediction will be saved to test_preds.pt.

Note: train.sh in the submission folder is only used for training Model 5.2. To train the model for other dataset, parameters of train.sh and test.sh need to be changed correspondingly.



